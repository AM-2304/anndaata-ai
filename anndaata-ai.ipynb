{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12798865,"sourceType":"datasetVersion","datasetId":8092198}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install virtualenv first (in Kaggle base python)\n!pip install --quiet virtualenv\n\n# Create env\n!virtualenv /kaggle/working/agri_env\n\n# Activate it\n!source /kaggle/working/agri_env/bin/activate\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- HARD RESET packaging in Kaggle ---\nimport sys, os, importlib.util, shutil\n\n!pip install -q --upgrade pip setuptools wheel\n\n# Pinned versions that avoid MRO conflict\n!pip install \"pydantic==1.10.*\" \"typing-extensions==4.12.2\" \"langchain==0.2.11\" \"langchain-community==0.2.10\"\n\n# Remove all packaging traces from site-packages\nfor p in sys.path:\n    if p.endswith(\"site-packages\") and os.path.exists(p):\n        for item in os.listdir(p):\n            if item.startswith(\"packaging\"):\n                try:\n                    target = os.path.join(p, item)\n                    if os.path.isdir(target):\n                        shutil.rmtree(target)\n                    else:\n                        os.remove(target)\n                except Exception as e:\n                    print(\"skip remove\", target, e)\n\n# Force reinstall clean version\n!pip install --no-cache-dir --force-reinstall packaging==20.0\n\n# Make sure Python picks the new one first\nsite_packages = \"/kaggle/working/agri_env/lib/python3.11/site-packages\"\nif site_packages not in sys.path:\n    sys.path.insert(0, site_packages)\n\n# Reload packaging\nimport packaging\nprint(\"‚úÖ Packaging fixed:\", packaging.__version__, \"from\", packaging.__file__)\n\n# Test Transformers now\nfrom transformers import pipeline\nprint(\"‚úÖ Transformers import works\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade pip setuptools wheel\n!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 \\\n  --extra-index-url https://download.pytorch.org/whl/cu121\n!pip install \"transformers>=4.46.1,<4.47.0\" sentence-transformers==3.2.1 accelerate==1.1.1 langchain-core\n!pip install langchain==0.3.7 langchain-community==0.3.7 chromadb==0.5.5\n!pip install unstructured==0.16.5 pdfminer.six==20240706 pillow==10.4.0 python-magic==0.4.27\n!pip install pandas==2.2.2 numpy==1.26.4 requests==2.32.3 tqdm httpx==0.28.1\n!pip install openai-whisper==20230314 parler-tts==0.2.2\n!pip install ipykernel \"scipy==1.11.4\" \n!pip install --force-reinstall --no-deps packaging==25.0\n!python -m ipykernel install --user --name=agri_env --display-name \"Python (agri_env)\"\n!pip install --quiet \\\n  numba==0.61.0 \\\n  tenacity==8.2.3 \\\n  pydantic==2.11.4 \\\n  google-api-core>=2.19 \\\n  \"google-cloud-bigquery-storage<3.0.0,>=2.30.0\" \\\n  fsspec==2025.3.2\n  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --force-reinstall --no-cache-dir \\\n  torch==2.3.1+cu121 torchvision==0.18.1+cu121 rich==12.4.4 torchaudio==2.3.1+cu121 rich==12.4.4 \\\n  --extra-index-url https://download.pytorch.org/whl/cu121\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CLEAN EVERYTHING CONFLICTING ---\n!pip uninstall -y pydantic pydantic-core langchain langchain-core langchain-community fastapi datamodel-code-generator\n\n# --- INSTALL COMPATIBLE VERSIONS (pre-Pydantic v2) ---\n!pip install \"pydantic==1.10.8\"\n!pip install \"langchain==0.0.352\"\n!pip install \"langchain-core==0.1.4\"\n!pip install \"langchain-community==0.0.21\"\n\n# --- REINSTALL REST OF STACK ---\n!pip install transformers==4.39.3 sentence-transformers==2.6.1\n!pip install librosa==0.10.1 soundfile==0.12.1 opencv-python==4.9.0.80\n!pip install git+https://github.com/huggingface/parler-tts.git\n!pip install whisper-openai whisper\n!pip install python-dotenv requests tqdm\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1) Purge every conflicting install\n!pip uninstall -y pydantic pydantic-core langchain langchain-core langchain-community fastapi\n\n# 3) Reinstall everything with constraints\n!pip install \"langchain<=0.0.352\" \\\n            \"langchain-core<=0.0.15\" \\\n            \"langchain-community<=0.0.17\" \\\n            \"transformers==4.39.3\" \\\n            \"sentence-transformers==2.6.1\" \n            \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y torch torchaudio torchvision\n!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 \n!pip install \"transformers==4.46.1\" \"sentence-transformers==3.2.1\" accelerate==1.1.1\n!pip install \"langchain==0.3.7\" \"langchain-community==0.3.7\" \"chromadb==0.5.5\"\n!pip install openai-whisper==20230314 parler-tts==0.2.2\n!pip install spacy==3.7.5 nltk==3.9.1 gensim==4.3.3\n!pip install librosa==0.10.2 soundfile==0.12.1 pydub==0.25.1 opencv-python==4.10.0.84\n!pip install requests==2.32.3 httpx==0.28.1 tqdm==4.66.5 python-dotenv==1.0.1\n\n# --- Imports ---\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Torch / Transformers\nimport torch\nfrom transformers import pipeline, AutoTokenizer, AutoModel\n\n# LangChain + Chroma (community versions!)\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.schema import Document\n\n# Whisper + TTS\nimport whisper\nfrom parler_tts import ParlerTTSForConditionalGeneration\n\n# NLP\nimport spacy, nltk, gensim\n\n# Audio / CV (avoid torchaudio dependency issues)\nimport librosa, soundfile as sf\nfrom pydub import AudioSegment\nimport cv2\n\n# Utils\nimport requests, httpx, tqdm, os, sys, platform\nfrom dotenv import load_dotenv\n\n# --- Debug: show runtime versions ---\nprint(\"CUDA available:\", torch.cuda.is_available(),\n      \"| Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\nprint(\"torch:\", torch.__version__,\n      \"| transformers:\", __import__('transformers').__version__,\n      \"| langchain:\", __import__('langchain').__version__)\nprint(\"Python:\", sys.version,\n      \"| OS:\", platform.platform())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Imports ---\n!pip uninstall -y ydata-profiling whisper torch torchaudio torchvision\n!pip install --quiet tenacity==8.2.3 langchain-openai==0.3.27 langchain_community langchain_core audiotoolbox pydub audio_utils\n!pip install git+https://github.com/huggingface/parler-tts.git\n!pip install torch==2.0.1 torchaudio==2.0.1 --extra-index-url https://download.pytorch.org/whl/cu117\n\n# Core\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Torch / Transformers\nimport torch\nfrom transformers import pipeline, AutoTokenizer, AutoModel\n\n# LangChain + Chroma\nfrom langchain import hub\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.schema import Document\n\n# Whisper + TTS\nimport whisper\nfrom parler_tts import ParlerTTSForConditionalGeneration\nfrom transformers import AutoTokenizer\n\n# NLP\nimport spacy, nltk, gensim\n\n# Audio / CV\nimport librosa, soundfile as sf, cv2\n\n# Google / BigQuery\nfrom google.cloud import bigquery\n\n# Utils\nimport requests, httpx, tqdm, os, sys, platform\nfrom dotenv import load_dotenv\n\n\n# --- Debug: show runtime versions ---\nprint(\"CUDA available:\", torch.cuda.is_available(), \n      \"| Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\nprint(\"torch:\", torch.__version__, \n      \"| Python:\", sys.version, \n      \"| OS:\", platform.platform())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install whisper","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nBASE = Path(\"/kaggle/working/agri_intellect\")\nfor p in [BASE/\"data\", BASE/\"chroma_db\", BASE/\"cache\", BASE/\"outputs\"]:\n    p.mkdir(parents=True, exist_ok=True)\n\n# (Optional) copy any small uploaded files from /kaggle/input into working space\n# Example: copy PDFs into our local data folder so unstructured can write sidecar files\nimport shutil, os\nIN_ROOT = Path(\"/kaggle/input/agri-intellect\")\nfor ds in IN_ROOT.iterdir():\n    # pull common doc files to our data dir (skip big image datasets)\n    for ext in (\".pdf\", \".csv\", \".xlsx\", \".xls\"):\n        for f in ds.rglob(f\"*{ext}\"):\n            dest = BASE/\"data\"/f.name\n            if not dest.exists():\n                shutil.copy2(f, dest)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\n# Fetch secrets safely\nos.environ[\"OPEN_AI_API_KEY\"] = user_secrets.get_secret(\"OPEN_AI_API_KEY\")\nos.environ[\"WEATHER_API\"] = user_secrets.get_secret(\"WEATHER_API\")\nos.environ[\"HUGGING_FACE\"] = user_secrets.get_secret(\"HUGGING_FACE\")\nos.environ[\"SERPAPI_KEY\"] = user_secrets.get_secret(\"SERPAPI_KEY\")\nprint(\"‚úÖ Keys loaded from Kaggle Secrets:\")\nfor k in [\"OPEN_AI_API_KEY\", \"WEATHER_API\", \"HUGGING_FACE\", \"SERPAPI_KEY\"]:\n    print(k, \"=\", \"‚úîÔ∏è set\" if os.getenv(k) else \"‚ùå missing\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# üîπ Clean uninstall of unstructured-related packages\n!pip uninstall -y unstructured unstructured-inference pdfminer pdfminer.six pdfplumber\n\n# üîπ Install necessary replacements\n!pip install pypdf==5.1.0 pymupdf==1.24.10\n!pip install langchain langchain-community chromadb\n!pip install sentence-transformers openpyxl xlrd\n\nfrom pathlib import Path\nimport pandas as pd\nfrom langchain.schema import Document\nfrom langchain_community.document_loaders import CSVLoader, PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# Paths\nDATA_DIR = Path(\"/kaggle/input/agri-intellect/agri_intellect\")\nDB_DIR   = Path(\"/kaggle/working/agri_intellect/chroma_db\")\n\n# 1) Load PDFs\npdf_docs = []\nfor f in DATA_DIR.rglob(\"*.pdf\"):\n    pdf_docs.extend(PyPDFLoader(str(f)).load())\n\n# 2) Load CSVs\ncsv_docs = []\nfor f in DATA_DIR.rglob(\"*.csv\"):\n    csv_docs.extend(CSVLoader(str(f)).load())\n\n# 3) Load Excel with pandas instead of UnstructuredExcelLoader\nexcel_docs = []\nfor f in list(DATA_DIR.rglob(\"*.xlsx\")) + list(DATA_DIR.rglob(\"*.xls\")):\n    df = pd.read_excel(f)\n    text = df.to_string()  # convert entire sheet to text\n    excel_docs.append(\n        Document(page_content=text, metadata={\"source\": str(f)})\n    )\n\n# Combine all docs\nall_docs = pdf_docs + csv_docs + excel_docs\n\n# 4) Split into chunks\nsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\nsplits = splitter.split_documents(all_docs)\n\n# 5) Embeddings + Chroma persistent store\nemb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\nvstore = Chroma.from_documents(splits, embedding=emb, persist_directory=str(DB_DIR))\nvstore.persist()\n\nprint(\"‚úÖ Documents processed and stored in ChromaDB\")\nprint(\"Total chunks:\", len(splits))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================\n# Agri-Intellect ‚Äî Full Integrated\n# (Multimodal RAG + VLM auto-switch CLIP <-> CDDM )\n# ===========================\n\nimport os, sys, json, time, math, sqlite3, traceback, re\nfrom pathlib import Path\nfrom typing import Optional, List, Dict, Tuple\nimport datetime\n\nimport requests\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\n# ML imports (assumes previously installed/pinned versions)\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer, util\n\n# LangChain + Chroma\nfrom langchain_community.vectorstores import Chroma\ntry:\n    from langchain_huggingface import HuggingFaceEmbeddings\nexcept Exception:\n    from langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain.docstore.document import Document\n\n# -----------------------\n# CONFIG & PATHS\n# -----------------------\nDATA_DIR = Path(\"/kaggle/input/agri-intellect/agri_intellect\")\nPEST_DIR = DATA_DIR / \"Pest_Dataset\"\nCHROMA_DIR = Path(\"/kaggle/working/agri_intellect/chroma_db\")\nPEST_EMB_CACHE = Path(\"/kaggle/working/pest_emb_dualmode.npz\")\nREMINDER_DB = Path(\"/kaggle/working/agri_reminders.db\")\nPESTICIDES_CSV = DATA_DIR / \"Pesticides.csv\"\n\n# Environment keys (set in Kaggle Secrets)\nWEATHER_API = os.environ.get(\"WEATHER_API\", \"\")\nSARVAM_API_KEY = os.environ.get(\"SARVAM_API_KEY\", \"\")\nSARVAM_BASE = os.environ.get(\"SARVAM_BASE\", \"https://api.sarvam.ai\")\nSERPAPI_KEY = os.environ.get(\"SERPAPI_KEY\", \"\")\nUSE_CDDM_FLAG = os.environ.get(\"USE_CDDM\", \"0\")  # \"1\" to prefer CDDM when possible\nCDDM_IMPORT_NAME = os.environ.get(\"CDDM_IMPORT\", \"cddm\")  # adapt if your CDDM lib import differs\n\n# Models\nEMB_MODEL_TEXT = \"sentence-transformers/all-MiniLM-L6-v2\"   # text embeddings for Chroma\nCLIP_MODEL = os.environ.get(\"CLIP_MODEL\", \"clip-ViT-B-32\")  # fallback clip\nGEN_MODEL = os.environ.get(\"GEN_MODEL\", \"google/flan-t5-small\")  # generator for RAG\nWHISPER_MODEL = os.environ.get(\"WHISPER_MODEL\", \"small\")\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nONLINE = True\ntry:\n    requests.get(\"https://www.google.com\", timeout=2)\nexcept Exception:\n    ONLINE = False\n\nprint(\"Device:\", DEVICE, \"| ONLINE:\", ONLINE, \"| USE_CDDM_FLAG:\", USE_CDDM_FLAG)\n\n# -----------------------\n# SAFE UTILITIES\n# -----------------------\ndef load_table(path: Path) -> pd.DataFrame:\n    if not path or not path.exists(): return pd.DataFrame()\n    try:\n        return pd.read_csv(path)\n    except Exception:\n        try:\n            return pd.read_excel(path)\n        except Exception:\n            return pd.DataFrame()\n\n# -----------------------\n# RAG: load retriever\n# -----------------------\ndef load_retriever(chroma_dir: Path = CHROMA_DIR, emb_model: str = EMB_MODEL_TEXT):\n    if not chroma_dir.exists():\n        raise FileNotFoundError(f\"Chroma DB not found at {chroma_dir}. Run parsing to produce it.\")\n    emb = HuggingFaceEmbeddings(model_name=emb_model)\n    v = Chroma(persist_directory=str(chroma_dir), embedding_function=emb)\n    return v, v.as_retriever(search_kwargs={\"k\":4})\n\n# -----------------------\n# GENERATOR (flan-t5)\n# -----------------------\n_GEN_TOKENIZER = None\n_GEN_MODEL = None\ndef load_generator(model_name: str = GEN_MODEL):\n    global _GEN_TOKENIZER, _GEN_MODEL\n    if _GEN_TOKENIZER is None or _GEN_MODEL is None:\n        _GEN_TOKENIZER = AutoTokenizer.from_pretrained(model_name)\n        _GEN_MODEL = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n        if DEVICE == \"cuda\":\n            _GEN_MODEL = _GEN_MODEL.to(\"cuda\")\n    return _GEN_MODEL, _GEN_TOKENIZER\n\ndef generate_from_context(question_en: str, docs: List[Document], max_new_tokens: int=150) -> str:\n    model, tok = load_generator()\n    ctx = \"\\n\\n\".join([d.page_content for d in docs])\n    prompt = (\n        \"You are Agri-Intellect. Use ONLY the context below to answer. If not in KB respond: 'I don‚Äôt know from the knowledge base.'\\n\\n\"\n        f\"Context:\\n{ctx}\\n\\nQuestion: {question_en}\\n\\nAnswer:\"\n    )\n    inputs = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n    if DEVICE == \"cuda\":\n        inputs = {k: v.to(\"cuda\") for k,v in inputs.items()}\n    out = model.generate(**inputs, max_new_tokens=max_new_tokens, num_beams=2, early_stopping=True)\n    return tok.decode(out[0], skip_special_tokens=True).strip()\n\n# -----------------------\n# Sarvam / ASR / TTS / Translation\n# -----------------------\ndef sarvam_translate(text: str, target_lang: str = \"en\") -> str:\n    if not SARVAM_API_KEY or not ONLINE: return text\n    try:\n        r = requests.post(f\"{SARVAM_BASE.rstrip('/')}/translate\", json={\"text\": text, \"target_lang\": target_lang},\n                          headers={\"Authorization\": f\"Bearer {SARVAM_API_KEY}\"}, timeout=12)\n        r.raise_for_status()\n        j = r.json()\n        return j.get(\"translation\") or j.get(\"translated_text\") or j.get(\"text\") or text\n    except Exception:\n        return text\n\ndef sarvam_asr(audio_path: str) -> str:\n    if not SARVAM_API_KEY or not ONLINE: return \"\"\n    try:\n        with open(audio_path,\"rb\") as f:\n            r = requests.post(f\"{SARVAM_BASE.rstrip('/')}/asr\", files={\"file\": f},\n                              headers={\"Authorization\": f\"Bearer {SARVAM_API_KEY}\"}, timeout=60)\n        r.raise_for_status()\n        return r.json().get(\"text\") or r.json().get(\"transcript\",\"\") or \"\"\n    except Exception:\n        return \"\"\n\ndef sarvam_tts(text: str, lang: str=\"hi\", out_path: str=\"/kaggle/working/sarvam_tts.wav\") -> Optional[str]:\n    if not SARVAM_API_KEY or not ONLINE: return None\n    try:\n        r = requests.post(f\"{SARVAM_BASE.rstrip('/')}/tts\", json={\"text\": text, \"lang\": lang},\n                          headers={\"Authorization\": f\"Bearer {SARVAM_API_KEY}\"}, timeout=60)\n        r.raise_for_status()\n        if r.headers.get(\"content-type\",\"\").startswith(\"audio\"):\n            with open(out_path,\"wb\") as f: f.write(r.content); return out_path\n        j = r.json(); b64 = j.get(\"audio_base64\") or j.get(\"audio\")\n        if b64:\n            import base64\n            with open(out_path,\"wb\") as f: f.write(base64.b64decode(b64)); return out_path\n    except Exception:\n        return None\n\n# offline Whisper / faster-whisper fallback\ndef local_asr_whisper(audio_path: str) -> str:\n    try:\n        from faster_whisper import WhisperModel\n        model = WhisperModel(WHISPER_MODEL, device=DEVICE)\n        segments, _ = model.transcribe(audio_path)\n        return \" \".join(s.text.strip() for s in segments)\n    except Exception:\n        try:\n            import whisper\n            m = whisper.load_model(WHISPER_MODEL)\n            r = m.transcribe(audio_path)\n            return r.get(\"text\",\"\").strip()\n        except Exception:\n            return \"\"\n\ndef local_tts(text: str, out_path: str=\"/kaggle/working/tts_local.wav\") -> Optional[str]:\n    try:\n        import pyttsx3\n        engine = pyttsx3.init()\n        engine.save_to_file(text, out_path)\n        engine.runAndWait()\n        return out_path\n    except Exception:\n        return None\n\ndef asr_transcribe(audio_path: str) -> str:\n    if SARVAM_API_KEY and ONLINE:\n        t = sarvam_asr(audio_path)\n        if t: return t\n    return local_asr_whisper(audio_path)\n\ndef tts_speak(text: str, lang: str=\"hi\") -> Optional[str]:\n    if SARVAM_API_KEY and ONLINE:\n        p = sarvam_tts(text, lang)\n        if p: return p\n    return local_tts(text)\n\ndef translate_to_en(text: str) -> str:\n    if SARVAM_API_KEY and ONLINE:\n        return sarvam_translate(text, target_lang=\"en\")\n    return text\n\ndef translate_from_en(text: str, tgt: str=\"hi\") -> str:\n    if tgt and tgt != \"en\" and SARVAM_API_KEY and ONLINE:\n        return sarvam_translate(text, target_lang=tgt)\n    return text\n\n# -----------------------\n# Intent detection (SBERT templates)\n# -----------------------\nINTENT_TEMPLATES = {\n    \"pest_image\": [\"identify pest\", \"what is this insect\", \"disease on leaves\"],\n    \"pest_text\": [\"what pesticide\", \"my leaves have\", \"insect on plant\"],\n    \"crop_choice\": [\"which crop to sow\", \"what crop is suitable\", \"what to plant\"],\n    \"weather\": [\"weather\", \"is it going to rain\", \"forecast\"],\n    \"msp\": [\"msp\", \"minimum support price\", \"government price\"],\n    \"irrigation\": [\"irrigate\", \"when to water\", \"how often to water\"],\n    \"fertilizer\": [\"fertilizer\", \"fertiliser\", \"how much fertilizer\"],\n    \"harvest\": [\"when to harvest\", \"harvest time\"],\n    \"storage\": [\"store\", \"storage\"],\n    \"livestock\": [\"livestock\", \"feeding\", \"vaccination\"],\n    \"schemes\": [\"scheme\", \"government scheme\", \"pm kisan\"],\n    \"reminder\": [\"remind me\", \"schedule\", \"set reminder\"],\n    \"general_rag\": [\"how to\", \"what is\", \"explain\", \"tell me about\"]\n}\n_INTENT_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n_intent_embeddings = {k: _INTENT_MODEL.encode(v, convert_to_tensor=True, normalize_embeddings=True) for k,v in INTENT_TEMPLATES.items()}\n\ndef detect_intent(text: str) -> Tuple[str, float]:\n    if not text: return \"general_rag\", 0.0\n    q = _INTENT_MODEL.encode(text, convert_to_tensor=True, normalize_embeddings=True)\n    best, best_score = None, -1.0\n    for k, emb in _intent_embeddings.items():\n        s = float(util.cos_sim(q, emb).max().item())\n        if s > best_score:\n            best_score = s; best = k\n    return best, best_score\n\n# -----------------------\n# VLMAdapter: CLIP <-> CDDM auto-switch\n# -----------------------\ndef _import_cddm_module():\n    if USE_CDDM_FLAG != \"1\": return None\n    try:\n        mod = __import__(CDDM_IMPORT_NAME)\n        print(\"Imported CDDM module:\", CDDM_IMPORT_NAME)\n        return mod\n    except Exception:\n        return None\n\n_CDDM_MOD = _import_cddm_module()\n_CDDM_AVAILABLE = (_CDDM_MOD is not None) and (DEVICE == \"cuda\")\nprint(\"CDDM available:\", bool(_CDDM_AVAILABLE))\n\nclass VLMAdapter:\n    def __init__(self, clip_model_name: str = CLIP_MODEL):\n        self.mode = \"clip\"\n        self.clip = None\n        self.cddm = None\n        self.text_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n        self._init(clip_model_name)\n\n    def _init(self, clip_model_name):\n        # try initialize CDDM placeholder\n        if _CDDM_AVAILABLE:\n            try:\n                # Replace the following with your CDDM model load if local.\n                # e.g., self.cddm = CDDM.load(...); self.mode='cddm'\n                # If your CDDM is server-side, set self.cddm = None and provide API calls in embed_image\n                self.cddm = getattr(_CDDM_MOD, \"CDDMModel\", None) or None\n                self.mode = \"cddm\"\n                print(\"VLMAdapter: attempting CDDM mode.\")\n            except Exception as e:\n                print(\"CDDM init failed:\", e)\n                self.cddm = None\n                self.mode = \"clip\"\n        else:\n            self.mode = \"clip\"\n        # CLIP fallback\n        try:\n            self.clip = SentenceTransformer(clip_model_name)\n            print(\"VLMAdapter: CLIP ready:\", clip_model_name)\n        except Exception as e:\n            print(\"VLMAdapter: CLIP init error:\", e)\n            self.clip = None\n\n    def set_mode(self, prefer_cddm: bool):\n        if prefer_cddm and _CDDM_AVAILABLE:\n            self.mode = \"cddm\"\n        else:\n            self.mode = \"clip\"\n        print(\"VLMAdapter mode set to:\", self.mode)\n\n    def embed_image(self, image_path: str) -> Optional[np.ndarray]:\n        # CDDM path (replace with your CDDM embedding interface)\n        if self.mode == \"cddm\" and self.cddm:\n            try:\n                # Example: if your local CDDM has embed_image:\n                if hasattr(self.cddm, \"embed_image\"):\n                    vec = self.cddm.embed_image(image_path)\n                    return np.asarray(vec, dtype=np.float32)\n                # If CDDM uses server API, implement call here (POST image -> vector)\n                # FALLBACK to CLIP if not implemented\n                raise NotImplementedError(\"CDDM embed not implemented; falling back to CLIP\")\n            except Exception as e:\n                print(\"CDDM embed failed:\", e)\n                return self._embed_with_clip(image_path)\n        # CLIP fallback\n        return self._embed_with_clip(image_path)\n\n    def _embed_with_clip(self, image_path: str) -> Optional[np.ndarray]:\n        if not self.clip:\n            raise RuntimeError(\"No CLIP available\")\n        try:\n            im = Image.open(image_path).convert(\"RGB\")\n            emb = self.clip.encode(im, convert_to_tensor=False, normalize_embeddings=True)\n            return np.asarray(emb, dtype=np.float32)\n        except Exception as e:\n            print(\"CLIP embed error:\", e); return None\n\n    def embed_text(self, text: str) -> Optional[np.ndarray]:\n        # If CDDM supports joint text-image embeddings, swap here\n        if self.mode == \"cddm\" and self.cddm and hasattr(self.cddm, \"embed_text\"):\n            try:\n                vec = self.cddm.embed_text(text)\n                return np.asarray(vec, dtype=np.float32)\n            except Exception as e:\n                print(\"CDDM embed_text failed:\", e)\n        # fallback SBERT\n        try:\n            emb = self.text_model.encode(text, convert_to_tensor=False, normalize_embeddings=True)\n            return np.asarray(emb, dtype=np.float32)\n        except Exception as e:\n            print(\"Text embed error:\", e); return None\n\nVLM = VLMAdapter()\n\n# -----------------------\n# PestIndex: cache embeddings for pest dataset (uses VLM)\n# -----------------------\ndef build_or_load_pest_index(pest_root: Path = PEST_DIR, cache_path: Path = PEST_EMB_CACHE):\n    if cache_path.exists():\n        try:\n            data = np.load(cache_path, allow_pickle=True)\n            idx = {k: [(p, np.array(e)) for p,e in data[k]] for k in data.files}\n            print(\"Loaded pest index from\", cache_path)\n            return idx\n        except Exception:\n            print(\"Failed to load pest cache; rebuilding.\")\n    index = {}\n    if not pest_root.exists(): return index\n    for pest_dir in sorted([p for p in pest_root.iterdir() if p.is_dir()]):\n        arr = []\n        for img in pest_dir.glob(\"*.*\"):\n            if img.suffix.lower() not in [\".jpg\",\".jpeg\",\".png\",\"bmp\",\"webp\"]: continue\n            try:\n                vec = VLM.embed_image(str(img))\n                if vec is None: continue\n                arr.append((str(img), vec.tolist()))\n            except Exception:\n                continue\n        if arr:\n            index[pest_dir.name] = arr\n    try:\n        np.savez(cache_path, **index)\n        print(\"Saved pest index to\", cache_path)\n    except Exception:\n        pass\n    return index\n\nPEST_INDEX = build_or_load_pest_index()\n\ndef identify_pest(query_image_path: str, per_class_limit: int=6) -> Optional[Tuple[str,float]]:\n    if not Path(query_image_path).exists(): return None\n    qvec = VLM.embed_image(query_image_path)\n    if qvec is None: return None\n    best = (None, -1.0)\n    for pest, refs in PEST_INDEX.items():\n        scores = []\n        for ppath, remb in refs[:per_class_limit]:\n            try:\n                s = float(util.cos_sim(torch.tensor(qvec), torch.tensor(np.array(remb))).item())\n                scores.append(s)\n            except Exception:\n                continue\n        if scores:\n            avg = float(np.mean(scores))\n            if avg > best[1]:\n                best = (pest, avg)\n    if best[0] is None: return None\n    return best\n\n# -----------------------\n# Pesticide / MSP loaders\n# -----------------------\nPESTICIDES_DF = load_table(PESTICIDES_CSV)\ndef lookup_pesticide_for(pest_name: str) -> Dict:\n    if PESTICIDES_DF.empty:\n        return {\"note\":\"Pesticides.csv missing\"}\n    df = PESTICIDES_DF\n    matches = df[df.apply(lambda row: row.astype(str).str.lower().str.contains(pest_name.lower()).any(), axis=1)]\n    if matches.empty:\n        col0 = df.columns[0]\n        matches = df[df[col0].astype(str).str.lower().str.contains(pest_name.lower())]\n    if matches.empty:\n        return {\"note\":\"No entry found\"}\n    row = matches.iloc[0].to_dict()\n    return {k:str(v) for k,v in row.items()}\n\ndef find_msp_table() -> Optional[Path]:\n    for p in DATA_DIR.rglob(\"*msp*.csv\"):\n        return p\n    return None\n\nMSP_PATH = find_msp_table()\nMSP_DF = load_table(MSP_PATH) if MSP_PATH else pd.DataFrame()\ndef lookup_msp(crop: str, yield_qty: Optional[float]=None) -> Dict:\n    if MSP_DF.empty:\n        return {\"note\":\"MSP table missing\"}\n    cropcol = next((c for c in MSP_DF.columns if \"crop\" in c.lower() or \"commodity\" in c.lower()), MSP_DF.columns[0])\n    pricecol = next((c for c in MSP_DF.columns if \"msp\" in c.lower() or \"price\" in c.lower()), None)\n    if not pricecol:\n        pricecol = MSP_DF.columns[1] if len(MSP_DF.columns)>1 else MSP_DF.columns[0]\n    sel = MSP_DF[MSP_DF[cropcol].astype(str).str.lower() == crop.lower()]\n    if sel.empty:\n        sel = MSP_DF[MSP_DF[cropcol].astype(str).str.lower().str.contains(crop.lower())]\n    if sel.empty:\n        return {\"note\":\"MSP not found\"}\n    try:\n        price = float(str(sel.iloc[0][pricecol]).replace(\",\",\"\").strip())\n    except Exception:\n        price = sel.iloc[0][pricecol]\n    out = {\"crop\":crop, \"msp\":price}\n    if yield_qty is not None:\n        try: out[\"expected_revenue\"] = float(price)*float(yield_qty)\n        except Exception: out[\"expected_revenue\"] = None\n    return out\n\n# -----------------------\n# Agronomy heuristics (extendable)\n# -----------------------\nCROP_RULES = {\n    \"wheat\": {\"irrigation_days\":10, \"water_mm\":35, \"fertilizer\":\"N:120,P:60,K:40 kg/ha\", \"harvest_days\":120, \"storage\":\"Dry to 12% moisture\"},\n    \"rice\":  {\"irrigation_days\":5,  \"water_mm\":40, \"fertilizer\":\"N:100,P:60,K:40 kg/ha\", \"harvest_days\":135, \"storage\":\"Dry to 14% moisture\"},\n    \"maize\": {\"irrigation_days\":7,  \"water_mm\":35, \"fertilizer\":\"N:120,P:60,K:40 kg/ha\", \"harvest_days\":100, \"storage\":\"Dry to 13% moisture\"},\n}\n\ndef crop_agronomy(crop: str) -> Dict:\n    k = crop.strip().lower()\n    return CROP_RULES.get(k, {\"note\":\"No heuristic available.\"})\n\n# -----------------------\n# Weather (indianapi.in) simple wrapper\n# -----------------------\ndef fetch_weather(city: str) -> Dict:\n    if not WEATHER_API or not ONLINE:\n        return {\"offline\":True, \"note\":\"Weather API not available\"}\n    try:\n        r = requests.get(\"https://indianapi.in/weather-api\", params={\"city\":city, \"key\":WEATHER_API}, timeout=10)\n        r.raise_for_status()\n        return r.json()\n    except Exception as e:\n        return {\"error\": str(e)}\n\ndef weather_recommendations(wx: Dict) -> List[Dict]:\n    out = []\n    for crop, meta in CROP_RULES.items():\n        out.append({\"crop\":crop, **meta})\n    return out\n\n# -----------------------\n# Reminders (SQLite)\n# -----------------------\ndef reminders_init():\n    conn = sqlite3.connect(str(REMINDER_DB))\n    cur = conn.cursor()\n    cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS reminders (id INTEGER PRIMARY KEY, task TEXT, next_due TEXT, interval_days INTEGER, meta TEXT)\"\"\")\n    conn.commit(); conn.close()\n\ndef schedule_reminder(task: str, start_in_days:int=0, interval_days:int=1, meta:Optional[Dict]=None):\n    reminders_init()\n    next_due = (datetime.datetime.now() + datetime.timedelta(days=int(start_in_days))).isoformat()\n    conn = sqlite3.connect(str(REMINDER_DB)); cur = conn.cursor()\n    cur.execute(\"INSERT INTO reminders (task,next_due,interval_days,meta) VALUES (?,?,?,?)\", (task, next_due, int(interval_days), json.dumps(meta or {})))\n    conn.commit(); conn.close()\n    return {\"task\":task, \"next_due\": next_due}\n\ndef check_reminders(now:Optional[datetime.datetime]=None):\n    reminders_init()\n    now = now or datetime.datetime.now()\n    conn = sqlite3.connect(str(REMINDER_DB)); cur = conn.cursor()\n    cur.execute(\"SELECT id,task,next_due,interval_days,meta FROM reminders\")\n    rows = cur.fetchall()\n    due = []\n    for r in rows:\n        rid, task, next_due_s, interval, meta = r\n        nd = datetime.datetime.fromisoformat(next_due_s)\n        if nd <= now:\n            due.append({\"id\":rid, \"task\":task, \"meta\": json.loads(meta)})\n            new_due = (now + datetime.timedelta(days=interval)).isoformat()\n            cur.execute(\"UPDATE reminders SET next_due=? WHERE id=?\", (new_due, rid))\n    conn.commit(); conn.close()\n    return due\n\n# -----------------------\n# SERPAPI fallback augmentation\n# -----------------------\ndef serpapi_snippets(query: str, num=2) -> Optional[str]:\n    if not SERPAPI_KEY or not ONLINE: return None\n    try:\n        r = requests.get(\"https://serpapi.com/search\", params={\"q\": query, \"api_key\": SERPAPI_KEY}, timeout=8)\n        r.raise_for_status()\n        js = r.json()\n        items = js.get(\"organic_results\") or []\n        snippets = []\n        for it in items[:num]:\n            s = it.get(\"snippet\") or it.get(\"title\")\n            if s: snippets.append(s)\n        if snippets: return \" \".join(snippets)\n    except Exception:\n        return None\n\n# -----------------------\n# HIGH-LEVEL USER ROUTER: handle_user_input\n# -----------------------\ndef ensure_retriever():\n    try:\n        v, retr = load_retriever(CHROMA_DIR)\n        return v, retr\n    except Exception:\n        return None, None\n\ndef handle_user_input(\n    text: Optional[str]=None,\n    audio_path: Optional[str]=None,\n    image_path: Optional[str]=None,\n    location: Optional[str]=None,\n    lang_hint: Optional[str]=None,\n    do_tts: bool=False\n) -> Dict:\n    \"\"\"\n    Single entry. Accepts text OR audio OR image.\n    Returns dict: {intent, answer_en, answer_local, meta...}\n    \"\"\"\n    user_lang = lang_hint or \"en\"\n    user_text = \"\"\n    if audio_path:\n        user_text = asr_transcribe(audio_path)\n    elif text:\n        user_text = str(text)\n    # PRIORITY: image diagnosis\n    if image_path:\n        # try multimodal chroma search first\n        try:\n            v, retr = ensure_retriever()\n            emb = VLM.embed_image(image_path)\n            docs = v.similarity_search_by_vector(emb.tolist(), k=4) if (v and emb is not None) else []\n        except Exception:\n            docs = []\n        pest = None\n        if docs:\n            pc = docs[0].page_content\n            if pc.lower().startswith(\"pest image of\"):\n                pest = pc.split(\"pest image of\")[-1].strip()\n        if not pest:\n            pid = identify_pest(image_path)\n            if pid:\n                pest, conf = pid\n        if pest:\n            pestic = lookup_pesticide_for(pest)\n            agr = crop_agronomy(pest) if pest.lower() in CROP_RULES else {}\n            ans_en = f\"Detected pest: {pest}. Pesticide info: {pestic}. Agronomy: {agr}\"\n        else:\n            ans_en = \"Could not identify pest from the image. Please upload a clearer photo or provide more details.\"\n        ans_local = translate_from_en(ans_en, user_lang)\n        if do_tts: tts_speak(ans_local, lang=user_lang)\n        return {\"intent\":\"pest_image\", \"answer_en\": ans_en, \"answer_local\": ans_local, \"pest\": pest}\n\n    # If no image: handle text-based intents\n    if not user_text:\n        return {\"error\":\"No input provided\"}\n\n    # translate to English for internal processing\n    text_en = translate_to_en(user_text)\n    intent, score = detect_intent(text_en)\n    if score < 0.35: intent = \"general_rag\"\n\n    # RAG retriever\n    try:\n        v, retriever = load_retriever(CHROMA_DIR)\n    except Exception:\n        v, retriever = None, None\n\n    # ROUTING\n    if intent == \"pest_text\":\n        # try extraction of pest phrase by checking folder names\n        pest_candidates = [p.name for p in PEST_DIR.iterdir() if p.is_dir()] if PEST_DIR.exists() else []\n        found = next((pc for pc in pest_candidates if pc.lower() in text_en.lower()), None)\n        if found:\n            pestic = lookup_pesticide_for(found)\n            agr = crop_agronomy(found) if found.lower() in CROP_RULES else {}\n            ans_en = f\"Pest: {found}. Pesticide: {pestic}. Agronomy: {agr}\"\n        else:\n            ans_en = \"I couldn't find pest name in your message. Please upload an image for accurate diagnosis.\"\n        ans_local = translate_from_en(ans_en, user_lang)\n        if do_tts: tts_speak(ans_local, lang=user_lang)\n        return {\"intent\":\"pest_text\", \"answer_en\": ans_en, \"answer_local\": ans_local}\n\n    if intent in (\"crop_choice\",\"weather\"):\n        if location:\n            wx = fetch_weather(location)\n            recs = weather_recommendations(wx)\n            lines = [f\"{r['crop'].title()}: irrigate every {r.get('irrigation_days','?')} days; fertilizer: {r.get('fertilizer','see dataset')}\" for r in recs]\n            ans_en = \"Weather: {}\\nRecommendations:\\n{}\".format((\"offline\" if wx.get(\"offline\") else \"live\"), \"\\n\".join(lines))\n        else:\n            ans_en = \"Please provide your location so I can fetch local weather and recommend crops.\"\n        ans_local = translate_from_en(ans_en, user_lang)\n        if do_tts: tts_speak(ans_local, lang=user_lang)\n        return {\"intent\":\"crop_choice\", \"answer_en\":ans_en, \"answer_local\":ans_local}\n\n    if intent == \"msp\":\n        # extract crop name heuristically\n        words = text_en.lower().split()\n        candidates = list(CROP_RULES.keys()) + (MSP_DF.iloc[:,0].astype(str).str.lower().tolist() if not MSP_DF.empty else [])\n        crop = next((c for c in candidates if c.lower() in text_en.lower()), None)\n        if not crop:\n            crop = words[-1]\n        yield_qty = None\n        for tok in words:\n            try:\n                if tok.replace(\".\",\"\",1).isdigit():\n                    yield_qty = float(tok); break\n            except: pass\n        res = lookup_msp(crop, yield_qty)\n        if \"note\" in res:\n            ans_en = res[\"note\"]\n        else:\n            ans_en = f\"MSP for {crop} is {res['msp']}. \" + (f\"Expected revenue: {res.get('expected_revenue')}\" if res.get('expected_revenue') else \"\")\n        ans_local = translate_from_en(ans_en, user_lang)\n        if do_tts: tts_speak(ans_local, lang=user_lang)\n        return {\"intent\":\"msp\", \"answer_en\":ans_en, \"answer_local\":ans_local, \"msp\":res}\n\n    if intent in (\"irrigation\",\"fertilizer\",\"harvest\",\"storage\",\"livestock\"):\n        found_crop = next((c for c in CROP_RULES.keys() if c.lower() in text_en.lower()), None)\n        if found_crop:\n            agr = crop_agronomy(found_crop)\n            if intent == \"irrigation\":\n                ans_en = f\"For {found_crop.title()}: irrigate every {agr.get('irrigation_days')} days (~{agr.get('water_mm')} mm).\"\n            elif intent == \"fertilizer\":\n                ans_en = f\"For {found_crop.title()}: recommended fertilizer: {agr.get('fertilizer')}\"\n            elif intent == \"harvest\":\n                ans_en = f\"Expected harvest for {found_crop.title()} in ~{agr.get('harvest_days')} days from sowing.\"\n            elif intent == \"storage\":\n                ans_en = f\"Storage: {agr.get('storage')}\"\n            else:\n                ans_en = \"Livestock: provide balanced feed, routine vaccination, clean housing, and monitor health.\"\n        else:\n            ans_en = \"Please mention the crop or livestock type for specific guidance.\"\n        ans_local = translate_from_en(ans_en, user_lang)\n        if do_tts: tts_speak(ans_local, lang=user_lang)\n        return {\"intent\":intent, \"answer_en\":ans_en, \"answer_local\":ans_local}\n\n    if intent == \"schemes\":\n        if retriever:\n            docs = retriever.get_relevant_documents(text_en)\n            if docs:\n                ans_en = generate_from_context(text_en, docs)\n            else:\n                s = serpapi_snippets(text_en)\n                ans_en = s or \"No matching scheme info found in knowledge base.\"\n        else:\n            ans_en = \"Knowledge base retriever not available.\"\n        ans_local = translate_from_en(ans_en, user_lang)\n        if do_tts: tts_speak(ans_local, lang=user_lang)\n        return {\"intent\":\"schemes\", \"answer_en\":ans_en, \"answer_local\":ans_local}\n\n    if intent == \"reminder\":\n        num = re.findall(r\"\\d+\", text_en)\n        interval = int(num[0]) if num else 1\n        task = text_en.split(\"to \",1)[1] if \"to \" in text_en else text_en\n        sched = schedule_reminder(task.strip(), start_in_days=0, interval_days=interval)\n        ans_en = f\"Reminder scheduled: {task.strip()} every {interval} day(s).\"\n        ans_local = translate_from_en(ans_en, user_lang)\n        return {\"intent\":\"reminder\", \"answer_en\":ans_en, \"answer_local\":ans_local, \"schedule\":sched}\n\n    # fallback: general RAG\n    if retriever:\n        docs = retriever.get_relevant_documents(text_en)\n        if docs:\n            ans_en = generate_from_context(text_en, docs)\n        else:\n            s = serpapi_snippets(text_en)\n            ans_en = s or \"I don't know from the knowledge base.\"\n    else:\n        ans_en = \"Knowledge base not available; please ensure Chroma DB exists.\"\n    ans_local = translate_from_en(ans_en, user_lang)\n    if do_tts: tts_speak(ans_local, lang=user_lang)\n    return {\"intent\":\"general_rag\", \"answer_en\":ans_en, \"answer_local\":ans_local}\n\n# -----------------------\n# Public helper APIs\n# -----------------------\ndef api_diagnose_pest(image_path: str, do_tts: bool=False) -> Dict:\n    pid = identify_pest(image_path)\n    if not pid:\n        return {\"detected\": None, \"message\":\"Could not identify pest.\"}\n    pest, conf = pid\n    pestic = lookup_pesticide_for(pest)\n    agr = crop_agronomy(pest) if pest.lower() in CROP_RULES else {}\n    msg = f\"Detected {pest} (confidence {conf:.2f}). Pesticide info: {pestic}. Agronomy: {agr}\"\n    local = translate_from_en(msg, \"hi\")\n    if do_tts: tts_speak(local, \"hi\")\n    return {\"detected\":pest, \"confidence\":conf, \"pesticide\":pestic, \"agronomy\":agr, \"message\":msg}\n\ndef api_crop_planning_by_city(city: str, do_tts: bool=False) -> Dict:\n    wx = fetch_weather(city)\n    recs = weather_recommendations(wx)\n    lines = [f\"{r['crop'].title()}: irrigate every {r.get('irrigation_days','?')} days; fertilizer {r.get('fertilizer','see dataset')}\" for r in recs]\n    msg = \"\\n\".join(lines)\n    local = translate_from_en(msg, \"hi\")\n    if do_tts: tts_speak(local, \"hi\")\n    return {\"weather\":wx, \"advice_en\":msg, \"advice_local\":local}\n\ndef api_msp_quote(crop: str, yield_qty: Optional[float]=None, do_tts: bool=False) -> Dict:\n    m = lookup_msp(crop, yield_qty)\n    if \"note\" in m:\n        local = translate_from_en(m[\"note\"], \"hi\")\n        return {\"error\":m[\"note\"], \"message_local\":local}\n    msg = f\"MSP for {crop} is {m['msp']}.\" + (f\" Expected revenue: {m.get('expected_revenue')}\" if m.get(\"expected_revenue\") else \"\")\n    local = translate_from_en(msg, \"hi\")\n    if do_tts: tts_speak(local, \"hi\")\n    return {\"msp\":m, \"message_local\":local}\n\ndef api_schedule(task:str, start_in_days:int=0, interval_days:int=1, meta:Optional[Dict]=None):\n    return schedule_reminder(task, start_in_days, interval_days, meta)\n\ndef api_check_reminders():\n    return check_reminders()\n\ndef set_vlm_mode(prefer_cddm: bool):\n    VLM.set_mode(prefer_cddm and _CDDM_AVAILABLE)\n    global PEST_INDEX\n    PEST_INDEX = build_or_load_pest_index()  # rebuild cache for consistency\n    return {\"mode\": VLM.mode, \"pest_classes\": len(PEST_INDEX)}\n\n# -----------------------\n# Final: ensure multimodal Chroma upsert (non-destructive)\n# -----------------------\ndef upsert_pest_images_into_chroma_if_needed(chroma_obj):\n    # Upsert images as documents with embeddings ‚Äî functionality depends on your Chroma client's API.\n    try:\n        count = 0\n        for pest_folder in sorted([p for p in PEST_DIR.iterdir() if p.is_dir()]):\n            for img in pest_folder.glob(\"*.*\"):\n                if img.suffix.lower() not in [\".jpg\",\".jpeg\",\".png\",\"bmp\",\"webp\"]: continue\n                emb = VLM.embed_image(str(img))\n                if emb is None: continue\n                # Use chroma object's add_texts (client-specific) with embeddings if available\n                try:\n                    chroma_obj.add_texts(\n                        texts=[f\"Pest image of {pest_folder.name}\"],\n                        metadatas=[{\"pest\": pest_folder.name, \"source\": str(img), \"type\":\"image\"}],\n                        embeddings=[emb.tolist()]\n                    )\n                    count += 1\n                except Exception:\n                    # some chroma clients don't accept direct embeddings; skip gracefully\n                    pass\n        if count>0:\n            chroma_obj.persist()\n        return count\n    except Exception:\n        return 0\n\n# Print readiness\nprint(\"Agri-Intellect integrated module ready.\")\nprint(\"Public functions: handle_user_input(), api_diagnose_pest(), api_crop_planning_by_city(), api_msp_quote(), api_schedule(), api_check_reminders(), set_vlm_mode(prefer_cddm)\")\nprint(\"Current VLM mode:\", VLM.mode)\n\n# Example: set_vlm_mode(False)  # force CLIP\n# Example: set_vlm_mode(True)   # prefer CDDM (if available)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\nimport tempfile\nimport os\nfrom typing import Optional, Dict\n\n# Assuming your entire Agri-Intellect code block is already defined above, including:\n# - handle_user_input()\n# - set_vlm_mode()\n# - tts_speak(), etc.\n\ndef gradio_agri_intellect(\n    text_input: str,\n    audio_file: Optional[gr.Audio.Audio] = None,\n    image_file: Optional[gr.Image.Image] = None,\n    location: str = \"\",\n    prefer_cddm: bool = False,\n    enable_tts: bool = False,\n    lang_hint: str = \"en\"\n) -> str:\n\n    # Save audio locally if provided\n    audio_path = None\n    if audio_file is not None:\n        with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as f:\n            f.write(audio_file.read())\n            audio_path = f.name\n\n    # Save image locally if provided\n    image_path = None\n    if image_file is not None:\n        img_temp = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")\n        image_file.save(img_temp.name)\n        image_path = img_temp.name\n\n    # Set VLM mode as per user toggle\n    set_vlm_mode(prefer_cddm)\n\n    # Run inference\n    try:\n        response = handle_user_input(\n            text=text_input,\n            audio_path=audio_path,\n            image_path=image_path,\n            location=location if location else None,\n            lang_hint=lang_hint,\n            do_tts=enable_tts\n        )\n    except Exception as e:\n        response = {\"error\": f\"Inference error: {str(e)}\"}\n\n    # Clean up temp files\n    if audio_path and os.path.exists(audio_path):\n        os.unlink(audio_path)\n    if image_path and os.path.exists(image_path):\n        os.unlink(image_path)\n\n    # Compose output string\n    output_lines = []\n\n    if \"error\" in response:\n        output_lines.append(f\"‚ùå Error: {response['error']}\")\n    else:\n        intent = response.get(\"intent\", \"unknown\")\n        output_lines.append(f\"üß† Detected Intent: {intent}\\n\")\n\n        answer_en = response.get(\"answer_en\") or response.get(\"message\") or \"\"\n        output_lines.append(f\"üá¨üáß Answer (English):\\n{answer_en}\\n\")\n\n        answer_local = response.get(\"answer_local\") or \"\"\n        if answer_local and answer_local != answer_en:\n            output_lines.append(f\"üåê Answer (Local Language - {lang_hint}):\\n{answer_local}\\n\")\n\n        # Optionally add extra info like pest name or MSP\n        if \"pest\" in response:\n            output_lines.append(f\"üêû Pest detected: {response['pest']}\\n\")\n        if \"msp\" in response:\n            output_lines.append(f\"üí∞ MSP Data: {response['msp']}\\n\")\n        if \"schedule\" in response:\n            sched = response[\"schedule\"]\n            output_lines.append(f\"‚è∞ Reminder Scheduled: Task '{sched.get('task')}' next due {sched.get('next_due')}\\n\")\n\n    return \"\\n\".join(output_lines)\n\n\nwith gr.Blocks(title=\"Agri-Intellect: Multimodal Agricultural Assistant\") as demo:\n    gr.Markdown(\n        \"\"\"\n        # Agri-Intellect\n        Multilingual, multimodal agricultural advisory system with:\n        - Text, Audio (speech), Image input\n        - Pest detection & pesticide suggestion\n        - Crop planning by weather and location\n        - MSP pricing info & harvesting/fertilizer/irrigation advice\n        - Reminder scheduling\n        - Auto-switch VLM (CDDM/CLIP)\n        - Multilingual ASR, TTS & translation support\n        \"\"\"\n    )\n\n    with gr.Row():\n        with gr.Column():\n            text_input = gr.Textbox(label=\"Enter your query (any language)\", lines=4, placeholder=\"Ask anything about agriculture...\")\n            audio_input = gr.Audio(source=\"microphone\", type=\"file\", label=\"Or speak your query\")\n            image_input = gr.Image(type=\"pil\", label=\"Upload crop or pest image (optional)\")\n            location_input = gr.Textbox(label=\"Your city/location (for weather-based advice)\", placeholder=\"e.g., Pune\")\n            lang_hint = gr.Dropdown(\n                label=\"Language of your query\",\n                choices=[\"en\", \"hi\", \"gu\", \"mr\", \"pa\", \"kn\", \"te\", \"ta\", \"bn\"],\n                value=\"en\",\n                interactive=True\n            )\n            prefer_cddm = gr.Checkbox(label=\"Prefer CDDM mode for pest detection (if available)\", value=False)\n            enable_tts = gr.Checkbox(label=\"Enable Text-to-Speech for responses\", value=False)\n            submit_btn = gr.Button(\"Get Agricultural Advice\")\n\n        with gr.Column():\n            output_box = gr.Textbox(label=\"Agri-Intellect Response\", lines=20, interactive=False)\n\n    submit_btn.click(\n        fn=gradio_agri_intellect,\n        inputs=[text_input, audio_input, image_input, location_input, prefer_cddm, enable_tts, lang_hint],\n        outputs=[output_box]\n    )\n\ndemo.launch(share=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}